#!/usr/bin/env python
"""This module puts together data from different sources that describe target safety liabilities."""

import logging
import sys
from typing import Optional

import fire
from pyspark import SparkFiles
from pyspark.conf import SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.dataframe import DataFrame
import pyspark.sql.functions as F

from common.evidence import detect_spark_memory_limit, write_evidence_strings

ADVERSE_EVENTS_URL = (
    'https://raw.githubusercontent.com/opentargets/curation/il-safety-curation/target_safety/adverse_effects.tsv'
)
SAFETY_RISK_INFO = (
    'https://raw.githubusercontent.com/opentargets/curation/il-safety-curation/target_safety/safety_risks.tsv'
)


def main(
    toxcast: str,
    output: str,
    adverse_events: str = ADVERSE_EVENTS_URL,
    safety_risk: str = SAFETY_RISK_INFO,
    log_file: Optional[str] = None,
):
    """
    This module puts together data from different sources that describe target safety liabilities.

    Args:
        adverse_events: Input TSV containing adverse events associated with targets that have been collected from relevant publications. Fetched from GitHub.
        safety_risk: Input TSV containing cardiovascular safety liabilities associated with targets that have been collected from relevant publications. Fetched from GitHub.
        toxcast: Input table containing biological processes associated with relevant targets that have been observed in toxicity assays.
        output: Output gzipped json file following the target safety liabilities data model.
        log_file: Destination of the logs generated by this script. Defaults to None.
    """

    # Logger initializer. If no log_file is specified, logs are written to stderr
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s %(levelname)s %(module)s - %(funcName)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
    )
    if log_file:
        logging.config.fileConfig(filename=log_file)
    else:
        logging.StreamHandler(sys.stderr)

    # Initialize spark context
    global spark
    spark = initialize_spark()
    spark.sparkContext.addFile(adverse_events)
    spark.sparkContext.addFile(safety_risk)
    logging.info('Remote files successfully added to the Spark Context.')

    # Load and process the input files into dataframes
    ae_df = process_adverse_events(SparkFiles.get(ADVERSE_EVENTS_URL.split('/')[-1]))
    sr_df = process_safety_risk(SparkFiles.get(SAFETY_RISK_INFO.split('/')[-1]))
    toxcast_df = process_toxcast(toxcast)
    logging.info('Data has been processed. Merging...')

    # Combine dfs and group evidence
    safety_df = (
        # dfs are combined; unionByName is used instead of union to address for the differences in the schemas
        ae_df.unionByName(sr_df, allowMissingColumns=True)
        .unionByName(toxcast_df, allowMissingColumns=True)
        # Group evidence by unique fields
        .groupBy('id', 'targetFromSourceId', 'datasource', 'event', 'eventId', 'effects', 'literature', 'url')
        .agg(
            F.collect_set(F.col('study')).alias('study'),
            F.collect_set(F.col('biosample')).alias('biosample'),
        )
    )

    # Write output
    logging.info('Evidence strings have been processed. Saving...')
    write_evidence_strings(safety_df, output)
    logging.info(f'{safety_df.count()} evidence of safety liabilities have been saved to {output}. Exiting.')

    return 0


def process_adverse_events(adverse_events: str) -> DataFrame:
    """
    Loads and processes the adverse events input TSV.

    Ex. input record:
        biologicalSystem | gastrointestinal
        effect           | activation_general
        efoId            | EFO_0009836
        ensemblId        | ENSG00000133019
        pmid             | 23197038
        ref              | Bowes et al. (2012)
        symptom          | bronchoconstriction
        target           | CHRM3
        uberonCode       | UBERON_0005409
        url              | null

    Ex. output record:
        id         | ENSG00000133019
        event      | bronchoconstriction
        datasource | Bowes et al. (2012)
        eventId    | EFO_0009836
        literature | 23197038
        url        | null
        biosample  | {gastrointestinal, UBERON_0005409, null, null, null}
        effects    | [{activation, general}]
    """

    ae_df = (
        spark.read.csv(adverse_events, sep='\t', header=True)
        .select(
            F.col('ensemblId').alias('id'),
            F.col('symptom').alias('event'),
            F.col('efoId').alias('eventId'),
            F.col('ref').alias('datasource'),
            F.col('pmid').alias('literature'),
            'url',
            F.struct(
                F.col('biologicalSystem').alias('tissueLabel'),
                F.col('uberonCode').alias('tissueId'),
                F.lit(None).alias('cellLabel'),
                F.lit(None).alias('cellFormat'),
                F.lit(None).alias('cellId'),
            ).alias('biosample'),
            F.split(F.col('effect'), '_').alias('effects'),
        )
        .withColumn(
            'effects',
            F.struct(
                F.element_at(F.col('effects'), 1).alias('direction'), F.element_at(F.col('effects'), 2).alias('dosing')
            ),
        )
    )

    # Multiple dosing effects need to be grouped in the same record.
    effects_df = ae_df.groupBy('id', 'event', 'datasource').agg(F.collect_set(F.col("effects")).alias("effects"))
    ae_df = ae_df.drop("effects").join(effects_df, on=["id", "event", "datasource"], how="left")

    return ae_df


def process_safety_risk(safety_risk: str) -> DataFrame:
    """
    Loads and processes the safety risk information input TSV.

    Ex. input record:
        biologicalSystem | cardiovascular sy...
        ensemblId        | ENSG00000132155
        event            | heart disease
        eventId          | EFO_0003777
        liability        | Important for the...
        pmid             | 21283106
        ref              | Force et al. (2011)
        target           | RAF1
        uberonId         | UBERON_0004535

    Ex. output record:
        id         | ENSG00000132155
        event      | heart disease
        eventId    | EFO_0003777
        literature | 21283106
        datasource | Force et al. (2011)
        biosample  | {cardiovascular s...
        study      | {Important for th...
    """

    return (
        spark.read.csv(safety_risk, sep='\t', header=True)
        .select(
            F.col('ensemblId').alias('id'),
            'event',
            'eventId',
            F.col('pmid').alias('literature'),
            F.col('ref').alias('datasource'),
            F.struct(
                F.col('biologicalSystem').alias('tissueLabel'),
                F.col('uberonId').alias('tissueId'),
                F.lit(None).alias('cellLabel'),
                F.lit(None).alias('cellFormat'),
                F.lit(None).alias('cellId'),
            ).alias('biosample'),
            F.struct(
                F.col('liability').alias('description'), F.lit(None).alias('name'), F.lit(None).alias('type')
            ).alias('study'),
        )
        .withColumn(
            'event',
            F.when(F.col('datasource').contains('Force'), 'heart disease').when(
                F.col('datasource').contains('Lamore'), 'cardiac arrhythmia'
            ),
        )
        .withColumn(
            'eventId',
            F.when(F.col('datasource').contains('Force'), 'EFO_0003777').when(
                F.col('datasource').contains('Lamore'), 'EFO_0004269'
            ),
        )
    )


def process_toxcast(toxcast: str) -> DataFrame:
    """
    Loads and processes the ToxCast input table.

    Ex. input record:
        assay_component_endpoint_name | ACEA_ER_80hr
        assay_component_desc          | ACEA_ER_80hr, is ...
        biological_process_target     | cell proliferation
        tissue                        | null
        cell_format                   | cell line
        cell_short_name               | T47D
        assay_format_type             | cell-based
        official_symbol               | ESR1
        eventId                       | null

    Ex. output record:
     targetFromSourceId | ESR1
    event              | cell proliferation
    eventId            | null
    biosample          | {null, null, T47D...
    datasource         | ToxCast
    url                | https://www.epa.g...
    study              | {ACEA_ER_80hr, AC...
    """

    return spark.read.csv(toxcast, sep='\t', header=True).select(
        F.trim(F.col('official_symbol')).alias('targetFromSourceId'),
        F.col('biological_process_target').alias('event'),
        'eventId',
        F.struct(
            F.col('tissue').alias('tissueLabel'),
            F.lit(None).alias('tissueId'),
            F.col('cell_short_name').alias('cellLabel'),
            F.col('cell_format').alias('cellFormat'),
            F.lit(None).alias('cellId'),
        ).alias('biosample'),
        F.lit('ToxCast').alias('datasource'),
        F.lit('https://www.epa.gov/chemical-research/exploring-toxcast-data-downloadable-data').alias('url'),
        F.struct(
            F.col('assay_component_endpoint_name').alias('name'),
            F.col('assay_component_desc').alias('description'),
            F.col('assay_format_type').alias('type'),
        ).alias('study'),
    )


def initialize_spark():
    """Spins up a Spark session."""

    # Initialize spark session
    spark_mem_limit = detect_spark_memory_limit()
    spark_conf = (
        SparkConf()
        .set('spark.driver.memory', f'{spark_mem_limit}g')
        .set('spark.executor.memory', f'{spark_mem_limit}g')
        .set('spark.driver.maxResultSize', '0')
        .set('spark.debug.maxToStringFields', '2000')
        .set('spark.sql.execution.arrow.maxRecordsPerBatch', '500000')
    )
    spark = SparkSession.builder.config(conf=spark_conf).master('local[*]').getOrCreate()
    logging.info(f'Spark version: {spark.version}')

    return spark


if __name__ == '__main__':
    fire.Fire(main)
