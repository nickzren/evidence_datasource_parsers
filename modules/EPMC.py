#!/usr/bin/env python

import argparse
import sys
import pyspark.sql
from pyspark.sql.types import *
from pyspark.sql.functions import *
import logging



def main():

    ##
    ## Parsing parameters:
    ##
    parser = argparse.ArgumentParser(description='This script generates target/disease evidence strings from ePMC cooccurrence files.')
    parser.add_argument('--cooccurrenceFile', help='Partioned parquet file with the ePMC cooccurrences', type=str, required=True)
    parser.add_argument('--outputFile', help='Resulting evidence file saved as compressed JSON.', type=str, required=True)
    parser.add_argument('--logFile', help='Destination of the logs generated by this script.', type=str, required=False)
    args = parser.parse_args()

    # extract parameters:  
    cooccurrenceFile = args.cooccurrenceFile


    # Initialize logger based on the provided logfile. 
    # If no logfile is specified, logs are written to stderr 
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s %(levelname)s %(module)s - %(funcName)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
    )
    if args.logFile:
        logging.config.fileConfig(filename=args.logFile)
    else:
        logging.StreamHandler(sys.stderr)

    # Parse output file:
    out_file = args.outputFile

    ##
    ## Initialize spark session
    ##
    global spark
    spark = (pyspark.sql.SparkSession.builder.getOrCreate())
    logging.info(f'Spark version: {spark.version}')

    ##
    ## Log parameters:
    ##
    logging.info(f'Cooccurrence file: {cooccurrenceFile}')
    logging.info(f'Output file: {out_file}')
    logging.info(f'Generating evidence:')

    ##
    ## Load/filter datasets
    ##

    (
        # Reading file:
        spark.read.parquet(cooccurrenceFile)

        # Filtering for diases/target cooccurrences:
        .filter(col('type') == "GP-DS")

        # PMIDs copied into a new column:
        .withColumn('tmp',col('pmid'))

        # Renaming columns:
        .withColumnRenamed("keywordId1", "targetFromSourceId")
        .withColumnRenamed("keywordId2", "diseaseFromSourceMappedId")

        # Aggregating data by publication, target and disease:
        .groupBy(['pmid', 'targetFromSourceId', 'diseaseFromSourceMappedId'])
        .agg(
            first(col("label1")).alias("targetFromSource"),
            first(col("label2")).alias("diseaseFromSource"),
            collect_set(col('tmp')).alias('literature'),
            collect_set(
                struct(  
                    col("text"),
                    col('start1').alias('tStart'),
                    col("end1").alias('tEnd'),
                    col('start2').alias('dStart'),
                    col("end2").alias('dEnd'), 
                    col('section')
                )
            ).alias('textMiningSentences')
        )

        # Adding linteral columns:
        .withColumn('datasourceId',lit('europepmc'))
        .withColumn('datatypeId',lit('literature'))

        # Dropping the original pmid column:
        .drop(*['pmid'])

        # Reorder columns:
        .select(["datasourceId", "datatypeId", "targetFromSource", "targetFromSourceId",
                "diseaseFromSource","diseaseFromSourceMappedId","literature","textMiningSentences"])

        # Save output:
        .write.format('json').mode('overwrite').option("compression", "org.apache.hadoop.io.compress.GzipCodec")
        .save('test.json.gz')
    )

    return 0


if __name__ == '__main__':

    main()

