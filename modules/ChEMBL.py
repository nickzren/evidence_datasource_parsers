#!/usr/bin/env python
"""This module adds the category of why a clinical trial has stopped early to the ChEMBL evidence."""

import argparse
import logging
import sys

import pyspark.sql.functions as f

from pyspark.sql.dataframe import DataFrame
from pyspark.sql.types import ArrayType, StringType

from common.evidence import initialize_sparksession, write_evidence_strings


def main(chembl_evidence: str, predictions: str, output_file: str) -> None:
    """
    This module adds the studyStopReasonCategories to the ChEMBL evidence as a result of the categorisation of the clinical trial reason to stop.
    Args:
        chembl_evidence: Input gzipped JSON with the evidence submitted by ChEMBL.
        predictions: Input JSON containing the categories of the clinical trial reason to stop. 
        Instructions for applying the ML model here: https://github.com/ireneisdoomed/stopReasons.
        output_file: Output gzipped json file containing the ChEMBL evidence with the additional studyStopReasonCategories field.
        log_file: Destination of the logs generated by this script. Defaults to None.
    """
    logging.info(f'ChEMBL evidence JSON file: {chembl_evidence}')
    logging.info(f'Classes of reason to stop table: {predictions}')

    chembl_df = spark.read.json(chembl_evidence).repartition(200).persist()
    predictions_df = (
        spark.read.json(predictions)
        .select('nct_id', f.col('subclasses').alias('studyStopReasonCategories'))
        .distinct()
    )

    # Join datasets
    early_stopped_evd_df = (
        # Evidence with a given reason to stop is always supported by a single NCT ID
        chembl_df.filter(f.col('studyStopReason').isNotNull())
        .select(
            "*", f.explode(f.col("urls.url")).alias("nct_id")
        ).withColumn("nct_id", f.element_at(f.split(f.col("nct_id"), "%22"), -2))
        .join(predictions_df, on='nct_id', how='left').drop('nct_id')
        .distinct()
    )

    # We expect that ~10% of evidence strings have a reason to stop assigned
    # It is asserted that this fraction is between 9 and 11% of the total count
    total_count = chembl_df.count()
    early_stopped_count = early_stopped_evd_df.count()

    if not 0.08 < early_stopped_count / total_count < 0.11:
        raise AssertionError(f'The fraction of evidence with a CT reason to stop class is not as expected ({early_stopped_count / total_count}).')

    logging.info('Evidence strings have been processed. Saving...')
    enriched_chembl_df = (
        chembl_df.filter(f.col('studyStopReason').isNull())
        .unionByName(early_stopped_evd_df, allowMissingColumns=True)
    )
    assert enriched_chembl_df.count() == chembl_df.count()
    write_evidence_strings(enriched_chembl_df, output_file)

    logging.info(f'{total_count} evidence strings have been saved to {output_file}. Exiting.')


def get_parser():
    """Get parser object for script ChEMBL.py."""
    parser = argparse.ArgumentParser(description=__doc__)

    parser.add_argument(
        '--chembl_evidence',
        help='Input gzipped JSON with the evidence submitted by ChEMBL',
        type=str,
        required=True,
    )
    parser.add_argument(
        '--predictions',
        help='Input TSV containing the categories of the clinical trial reason to stop. Instructions for applying the ML model here: https://github.com/ireneisdoomed/stopReasons.',
        type=str,
        required=True,
    )
    parser.add_argument(
        '--output', help='Output gzipped json file following the target safety liabilities data model.',
        type=str,
        required=True
    )
    parser.add_argument(
        '--log_file',
        help='Destination of the logs generated by this script. Defaults to None',
        type=str,
        nargs='?',
        default=None
    )

    return parser


if __name__ == '__main__':
    args = get_parser().parse_args()

    # Logger initializer. If no log_file is specified, logs are written to stderr
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s %(levelname)s %(module)s - %(funcName)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
    )
    if args.log_file:
        logging.config.fileConfig(filename=args.log_file)
    else:
        logging.StreamHandler(sys.stderr)

    global spark
    spark = initialize_sparksession()

    main(
        chembl_evidence=args.chembl_evidence,
        predictions=args.predictions,
        output_file=args.output
    )
