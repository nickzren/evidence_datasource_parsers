#!/usr/bin/env python

import sys
import argparse
import gzip
import logging
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

class SLAPEnrichEvidenceGenerator():
    def __init__(self):
        # Create spark session     
        self.spark = (SparkSession.builder
                .appName('SLAPEnrich')
                .getOrCreate())
        logging.info(f"Spark version: {self.spark.version}")

        # Initialize source table
        self.dataframe = None

    def generateEvidenceFromSource(self, inputFile, mapping2EFO, skipMapping):
        '''
        Processing of the input file to build all the evidences from its data
        Returns:
            evidences (array): Object with all the generated evidences strings from source file
        '''
        # Read input file
        self.dataframe = (self.spark
                        .read.csv(inputFile, sep=r'\t', header=True, inferSchema=True)
                        .select("ctype", "gene", "pathway", "SLAPEnrichPval")
                        .withColumnRenamed("ctype", "Cancer_type_acronym")
                        .withColumnRenamed("SLAPEnrichPval", "pval")
                        .withColumn("pathwayId", split(col("pathway"), ": ").getItem(0))
                        .withColumn("pathwayDescription", split(col("pathway"), ": ").getItem(1)))

        # Filter by p-value
        self.dataframe = self.dataframe.filter(col("pval") < 1e-4) 

        # Mapping step
        if not skipMapping:
            try:
                self.dataframe = self.cancer2EFO(mapping2EFO)
                logging.info("Disease mappings have been imported.")
            except Exception as e:
                logging.error(f"An error occurred while importing disease mappings: \n{e}.")

        # Build evidence strings per row
        logging.info("Generating evidence:")
        evidences = (self.dataframe.rdd
            .map(SLAPEnrichEvidenceGenerator.parseEvidenceString)
            .collect()) # list of dictionaries
        
        return evidences
    
    def cancer2EFO(self, mapping2EFO):
        diseaseMappingsFile = (self.spark
                        .read.csv(mapping2EFO, sep=r'\t', header=True)
                        .select("Cancer_type_acronym", "EFO_id"))

        self.dataframe = self.dataframe.join(
            diseaseMappingsFile,
            on="Cancer_type_acronym",
            how="inner"
        )

        return self.dataframe

    @staticmethod
    def parseEvidenceString(row):
        try:
            evidence = {
                "datasourceId" : "slapenrich",
                "datatypeId" : "affected_pathway",
                "resourceScore" : row["pval"],
                "pathwayName" : row["pathwayDescription"],
                "pathwayId" : row["pathwayId"],
                "targetFromSourceId" : row["gene"],
                "diseaseFromSource" : row["Cancer_type_acronym"]
            }
            if "EFO_id" in row:
                evidence["diseaseFromSourceMappedId"] = row["EFO_id"]
            return evidence
        except Exception as e:
            raise        

def main():
    # Initiating parser
    parser = argparse.ArgumentParser(description=
    "This script generates evidences for the SLAPEnrich data source.")

    parser.add_argument("-i", "--inputFile", required=True, type=str, help="Input source .tsv file.")
    parser.add_argument("-m", "--mapping2EFO", required=True, type=str, help="Input look-up table containing the cancer type mappings to an EFO ID.")
    parser.add_argument("-o", "--outputFile", required=True, type=str, help="Gzipped JSON file containing the evidence strings.")
    parser.add_argument("-s", "--skipMapping", required=False, action="store_true", help="State whether to skip the disease to EFO mapping step.")
    parser.add_argument("-l", "--logFile", help="Destination of the logs generated by this script.", type=str, required=False)

    # Parsing parameters
    args = parser.parse_args()
    inputFile = args.inputFile
    mapping2EFO = args.mapping2EFO
    outputFile = args.outputFile
    skipMapping = args.skipMapping

    # Initialize logging:
    logging.basicConfig(
    level=logging.INFO,
    format='%(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    )
    if args.logFile:
        logging.config.fileConfig(filename=args.logFile)
    else:
        logging.StreamHandler(sys.stderr)

    # Logging parameters
    logging.info(f"SLAPEnrich input table: {inputFile}")
    logging.info(f"Cancer type to EFO ID table: {mapping2EFO}")
    logging.info(f"Output file: {outputFile}")

    # Initialize evidence builder object
    evidenceBuilder = SLAPEnrichEvidenceGenerator()

    # Writing evidence strings into a json file
    evidences = evidenceBuilder.generateEvidenceFromSource(inputFile, mapping2EFO, skipMapping)

    with gzip.open(outputFile, "wt") as f:
        for evidence in evidences:
            json.dump(evidence, f)
            f.write('\n')
    logging.info(f"{len(evidences)} evidence strings saved into {outputFile}. Exiting.")

if __name__ == '__main__':
    main()